{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHeFwKZd6P6P"
   },
   "source": [
    "# Gradient Centralization for Better Training Performance\n",
    "\n",
    "**Author:** [Rishit Dagli](https://github.com/Rishit-dagli)<br>\n",
    "**Date created:** 06/18/21<br>\n",
    "**Last modified:** 06/18/21<br>\n",
    "**Description:** Implement Gradient Centralization to improve training performance of DNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7axVrCG6P6S"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This example implements [Gradient Centralization](https://arxiv.org/abs/2004.01461), a\n",
    "new optimization technique for Deep Neural Networks by Yong et al., and demonstrates it\n",
    "on Laurence Moroney's [Horses or Humans\n",
    "Dataset](https://www.tensorflow.org/datasets/catalog/horses_or_humans). Gradient\n",
    "Centralization can both speedup training process and improve the final generalization\n",
    "performance of DNNs. It operates directly on gradients by centralizing the gradient\n",
    "vectors to have zero mean. Gradient Centralization morever improves the Lipschitzness of\n",
    "the loss function and its gradient so that the training process becomes more efficient\n",
    "and stable.\n",
    "\n",
    "This example requires TensorFlow 2.2 or higher as well as `tensorflow_datasets` which can\n",
    "be installed with this command:\n",
    "\n",
    "```\n",
    "pip install tensorflow-datasets\n",
    "```\n",
    "\n",
    "We will be implementing Gradient Centralization in this example but you could also use\n",
    "this very easily with a package I built,\n",
    "[gradient-centralization-tf](https://github.com/Rishit-dagli/Gradient-Centralization-TensorFlow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2x-g8iAg6P6S"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Eg1nP3rz6P6T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 11:46:01.606207: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SJ1JAIs6P6T"
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "For this example, we will be using the [Horses or Humans\n",
    "dataset](https://www.tensorflow.org/datasets/catalog/horses_or_humans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3TQMCWUW6P6T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 11:46:03.239808: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 153.59 MiB (download: 153.59 MiB, generated: Unknown size, total: 153.59 MiB) to /home/zlabs-hwa-01/tensorflow_datasets/horses_or_humans/3.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ab54df3f284ff7b46883a699e0e295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1824d9109e4699a3f1ff78f447bbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/1027 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/zlabs-hwa-01/tensorflow_datasets/horses_or_humans/3.0.0.incompleteHIZAMO/horses_or_humans-trai…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/zlabs-hwa-01/tensorflow_datasets/horses_or_humans/3.0.0.incompleteHIZAMO/horses_or_humans-test…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset horses_or_humans downloaded and prepared to /home/zlabs-hwa-01/tensorflow_datasets/horses_or_humans/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "Image shape: (300, 300, 3)\n",
      "Training images: 1027\n",
      "Test images: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 11:46:25.069331: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2023-06-20 11:46:25.069361: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: zlabshwa01\n",
      "2023-06-20 11:46:25.069366: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: zlabshwa01\n",
      "2023-06-20 11:46:25.069445: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 530.30.2\n",
      "2023-06-20 11:46:25.069462: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.105.1\n",
      "2023-06-20 11:46:25.069465: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 515.105.1 does not match DSO version 530.30.2 -- cannot find working devices in this configuration\n",
      "2023-06-20 11:46:25.070231: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "input_shape = (300, 300, 3)\n",
    "dataset_name = \"horses_or_humans\"\n",
    "batch_size = 128\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "(train_ds, test_ds), metadata = tfds.load(\n",
    "    name=dataset_name,\n",
    "    split=[tfds.Split.TRAIN, tfds.Split.TEST],\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")\n",
    "\n",
    "print(f\"Image shape: {metadata.features['image'].shape}\")\n",
    "print(f\"Training images: {metadata.splits['train'].num_examples}\")\n",
    "print(f\"Test images: {metadata.splits['test'].num_examples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pu1RwMzZ6P6U"
   },
   "source": [
    "## Use Data Augmentation\n",
    "\n",
    "We will rescale the data to `[0, 1]` and perform simple augmentations to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lF2MrVpJ6P6U"
   },
   "outputs": [],
   "source": [
    "rescale = layers.Rescaling(1.0 / 255)\n",
    "\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        layers.RandomRotation(0.3),\n",
    "        layers.RandomZoom(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def prepare(ds, shuffle=False, augment=False):\n",
    "    # Rescale dataset\n",
    "    ds = ds.map(lambda x, y: (rescale(x), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1024)\n",
    "\n",
    "    # Batch dataset\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    # Use data augmentation only on the training set\n",
    "    if augment:\n",
    "        ds = ds.map(\n",
    "            lambda x, y: (data_augmentation(x, training=True), y),\n",
    "            num_parallel_calls=AUTOTUNE,\n",
    "        )\n",
    "\n",
    "    # Use buffered prefecting\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyT59RSo6P6U"
   },
   "source": [
    "Rescale and augment the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lz8jlFMF6P6U"
   },
   "outputs": [],
   "source": [
    "train_ds = prepare(train_ds, shuffle=True, augment=True)\n",
    "test_ds = prepare(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck2V8hYg6P6V"
   },
   "source": [
    "## Define a model\n",
    "\n",
    "In this section we will define a Convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ktkxWXGx6P6V"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(300, 300, 3)),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xU7ZA-s6P6V"
   },
   "source": [
    "## Implement Gradient Centralization\n",
    "\n",
    "We will now\n",
    "subclass the `RMSProp` optimizer class modifying the\n",
    "`tf.keras.optimizers.Optimizer.get_gradients()` method where we now implement Gradient\n",
    "Centralization. On a high level the idea is that let us say we obtain our gradients\n",
    "through back propogation for a Dense or Convolution layer we then compute the mean of the\n",
    "column vectors of the weight matrix, and then remove the mean from each column vector.\n",
    "\n",
    "The experiments in [this paper](https://arxiv.org/abs/2004.01461) on various\n",
    "applications, including general image classification, fine-grained image classification,\n",
    "detection and segmentation and Person ReID demonstrate that GC can consistently improve\n",
    "the performance of DNN learning.\n",
    "\n",
    "Also, for simplicity at the moment we are not implementing gradient cliiping functionality,\n",
    "however this quite easy to implement.\n",
    "\n",
    "At the moment we are just creating a subclass for the `RMSProp` optimizer\n",
    "however you could easily reproduce this for any other optimizer or on a custom\n",
    "optimizer in the same way. We will be using this class in the later section when\n",
    "we train a model with Gradient Centralization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "odMgqSsn6P6V"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GCRMSprop(RMSprop):\n",
    "    def get_gradients(self, loss, params):\n",
    "        # We here just provide a modified get_gradients() function since we are\n",
    "        # trying to just compute the centralized gradients.\n",
    "\n",
    "        grads = []\n",
    "        gradients = super().get_gradients()\n",
    "        for grad in gradients:\n",
    "            grad_len = len(grad.shape)\n",
    "            if grad_len > 1:\n",
    "                axis = list(range(grad_len - 1))\n",
    "                grad -= tf.reduce_mean(grad, axis=axis, keep_dims=True)\n",
    "            grads.append(grad)\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "optimizer = GCRMSprop(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUsABl9A6P6V"
   },
   "source": [
    "## Training utilities\n",
    "\n",
    "We will also create a callback which allows us to easily measure the total training time\n",
    "and the time taken for each epoch since we are interested in comparing the effect of\n",
    "Gradient Centralization on the model we built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lds6WLJ36P6V"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TimeHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time() - self.epoch_time_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHg3MQfo6P6V"
   },
   "source": [
    "## Train the model without GC\n",
    "\n",
    "We now train the model we built earlier without Gradient Centralization which we can\n",
    "compare to the training performance of the model trained with Gradient Centralization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fOA8YdLs6P6W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 298, 298, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 149, 149, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 147, 147, 32)      4640      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 147, 147, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 73, 73, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 71, 71, 64)        18496     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 71, 71, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 35, 35, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 33, 33, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,704,097\n",
      "Trainable params: 1,704,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "time_callback_no_gc = TimeHistory()\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=RMSprop(learning_rate=1e-4),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLQy4_xB6P6W"
   },
   "source": [
    "We also save the history since we later want to compare our model trained with and not\n",
    "trained with Gradient Centralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efVDrvZS6P6W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 7s 532ms/step - loss: 0.6905 - accuracy: 0.5540\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 6s 535ms/step - loss: 0.6723 - accuracy: 0.5833\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 6s 538ms/step - loss: 0.6650 - accuracy: 0.6368\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 6s 531ms/step - loss: 0.6254 - accuracy: 0.6758\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 6s 518ms/step - loss: 0.5984 - accuracy: 0.7225\n",
      "Epoch 6/10\n",
      "7/9 [======================>.......] - ETA: 1s - loss: 0.5730 - accuracy: 0.7366"
     ]
    }
   ],
   "source": [
    "history_no_gc = model.fit(\n",
    "    train_ds, epochs=10, verbose=1, callbacks=[time_callback_no_gc]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAlvOo_A6P6W"
   },
   "source": [
    "## Train the model with GC\n",
    "\n",
    "We will now train the same model, this time using Gradient Centralization,\n",
    "notice our optimizer is the one using Gradient Centralization this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3s00z9X6P6W"
   },
   "outputs": [],
   "source": [
    "time_callback_gc = TimeHistory()\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history_gc = model.fit(train_ds, epochs=10, verbose=1, callbacks=[time_callback_gc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lePvK5E06P6W"
   },
   "source": [
    "## Comparing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtQcnfSt6P6W"
   },
   "outputs": [],
   "source": [
    "print(\"Not using Gradient Centralization\")\n",
    "print(f\"Loss: {history_no_gc.history['loss'][-1]}\")\n",
    "print(f\"Accuracy: {history_no_gc.history['accuracy'][-1]}\")\n",
    "print(f\"Training Time: {sum(time_callback_no_gc.times)}\")\n",
    "\n",
    "print(\"Using Gradient Centralization\")\n",
    "print(f\"Loss: {history_gc.history['loss'][-1]}\")\n",
    "print(f\"Accuracy: {history_gc.history['accuracy'][-1]}\")\n",
    "print(f\"Training Time: {sum(time_callback_gc.times)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2LjXeci6P6W"
   },
   "source": [
    "Readers are encouraged to try out Gradient Centralization on different datasets from\n",
    "different domains and experiment with it's effect. You are strongly advised to check out\n",
    "the [original paper](https://arxiv.org/abs/2004.01461) as well - the authors present\n",
    "several studies on Gradient Centralization showing how it can improve general\n",
    "performance, generalization, training time as well as more efficient.\n",
    "\n",
    "Many thanks to [Ali Mustufa Shaikh](https://github.com/ialimustufa) for reviewing this\n",
    "implementation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "gradient_centralization",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
