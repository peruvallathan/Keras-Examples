{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2nSieo881gj"
   },
   "source": [
    "# Self-supervised contrastive learning with NNCLR\n",
    "\n",
    "**Author:** [Rishit Dagli](https://twitter.com/rishit_dagli)<br>\n",
    "**Date created:** 2021/09/13<br>\n",
    "**Last modified:** 2021/09/13<br>\n",
    "**Description:** Implementation of NNCLR, a self-supervised learning method for computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMEe4E1381gl"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "### Self-supervised learning\n",
    "\n",
    "Self-supervised representation learning aims to obtain robust representations of samples\n",
    "from raw data without expensive labels or annotations. Early methods in this field\n",
    "focused on defining pretraining tasks which involved a surrogate task on a domain with ample\n",
    "weak supervision labels. Encoders trained to solve such tasks are expected to\n",
    "learn general features that might be useful for other downstream tasks requiring\n",
    "expensive annotations like image classification.\n",
    "\n",
    "### Contrastive Learning\n",
    "\n",
    "A broad category of self-supervised learning techniques are those that use *contrastive\n",
    "losses*, which have been used in a wide range of computer vision applications like\n",
    "[image similarity](https://www.jmlr.org/papers/v11/chechik10a.html),\n",
    "[dimensionality reduction (DrLIM)](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)\n",
    "and [face verification/identification](https://openaccess.thecvf.com/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html).\n",
    "These methods learn a latent space that clusters positive samples together while\n",
    "pushing apart negative samples.\n",
    "\n",
    "### NNCLR\n",
    "\n",
    "In this example, we implement NNCLR as proposed in the paper\n",
    "[With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations](https://arxiv.org/abs/2104.14548),\n",
    "by Google Research and DeepMind.\n",
    "\n",
    "NNCLR learns self-supervised representations that go beyond single-instance positives, which\n",
    "allows for learning better features that are invariant to different viewpoints, deformations,\n",
    "and even intra-class variations.\n",
    "Clustering based methods offer a great approach to go beyond single instance positives,\n",
    "but assuming the entire cluster to be positives could hurt performance due to early\n",
    "over-generalization. Instead, NNCLR uses nearest neighbors in the learned representation\n",
    "space as positives.\n",
    "In addition, NNCLR increases the performance of existing contrastive learning methods like\n",
    "[SimCLR](https://arxiv.org/abs/2002.05709)([Keras Example](https://keras.io/examples/vision/semisupervised_simclr))\n",
    "and reduces the reliance of self-supervised methods on data augmentation strategies.\n",
    "\n",
    "Here is a great visualization by the paper authors showing how NNCLR builds on ideas from\n",
    "SimCLR:\n",
    "\n",
    "![](https://i.imgur.com/p2DbZJJ.png)\n",
    "\n",
    "We can see that SimCLR uses two views of the same image as the positive pair. These two\n",
    "views, which are produced using random data augmentations, are fed through an encoder to\n",
    "obtain the positive embedding pair, we end up using two augmentations. NNCLR instead\n",
    "keeps a _support set_ of embeddings representing the full data distribution, and forms\n",
    "the positive pairs using nearest-neighbours. A support set is used as memory during\n",
    "training, similar to a queue (i.e. first-in-first-out) as in\n",
    "[MoCo](https://arxiv.org/abs/1911.05722).\n",
    "\n",
    "This example requires TensorFlow 2.6 or higher, as well as `tensorflow_datasets`, which can\n",
    "be installed with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4U6neCSZ81gl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (4.9.0)\n",
      "Requirement already satisfied: click in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (8.0.4)\n",
      "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (1.3.0)\n",
      "Requirement already satisfied: wrapt in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (1.14.1)\n",
      "Requirement already satisfied: absl-py in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (1.4.0)\n",
      "Requirement already satisfied: tqdm in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (4.65.0)\n",
      "Requirement already satisfied: numpy in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (1.23.5)\n",
      "Requirement already satisfied: toml in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: promise in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (2.29.0)\n",
      "Requirement already satisfied: protobuf in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (3.19.6)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (1.13.0)\n",
      "Requirement already satisfied: dm-tree in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (0.1.8)\n",
      "Requirement already satisfied: array-record in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (0.2.0)\n",
      "Requirement already satisfied: termcolor in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (2.3.0)\n",
      "Requirement already satisfied: psutil in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (5.9.0)\n",
      "Requirement already satisfied: typing_extensions in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (4.5.0)\n",
      "Requirement already satisfied: zipp in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (3.15.0)\n",
      "Requirement already satisfied: importlib_resources in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (5.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4)\n",
      "Requirement already satisfied: six in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.59.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5qzsmPy81gm"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "r9P7JkEm81gm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 21:36:28.043224: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1R2lXrlr81gm"
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "A greater `queue_size` most likely means better performance as shown in the original\n",
    "paper, but introduces significant computational overhead. The authors show that the best\n",
    "results of NNCLR are achieved with a queue size of 98,304 (the largest `queue_size` they\n",
    "experimented on). We here use 10,000 to show a working example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nFEz3BRp81gm"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "shuffle_buffer = 5000\n",
    "# The below two values are taken from https://www.tensorflow.org/datasets/catalog/stl10\n",
    "labelled_train_images = 5000\n",
    "unlabelled_images = 100000\n",
    "\n",
    "temperature = 0.1\n",
    "queue_size = 10000\n",
    "contrastive_augmenter = {\n",
    "    \"brightness\": 0.5,\n",
    "    \"name\": \"contrastive_augmenter\",\n",
    "    \"scale\": (0.2, 1.0),\n",
    "}\n",
    "classification_augmenter = {\n",
    "    \"brightness\": 0.2,\n",
    "    \"name\": \"classification_augmenter\",\n",
    "    \"scale\": (0.5, 1.0),\n",
    "}\n",
    "input_shape = (96, 96, 3)\n",
    "width = 128\n",
    "num_epochs = 25\n",
    "steps_per_epoch = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5878eC6A81gn"
   },
   "source": [
    "## Load the Dataset\n",
    "\n",
    "We load the [STL-10](http://ai.stanford.edu/~acoates/stl10/) dataset from\n",
    "TensorFlow Datasets, an image recognition dataset for developing unsupervised\n",
    "feature learning, deep learning, self-taught learning algorithms. It is inspired by the\n",
    "CIFAR-10 dataset, with some modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ei-QyQbf81gn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 21:36:29.167236: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2023-06-27 21:36:29.167309: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: zlabshwa01\n",
      "2023-06-27 21:36:29.167324: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: zlabshwa01\n",
      "2023-06-27 21:36:29.167532: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 530.30.2\n",
      "2023-06-27 21:36:29.167576: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.105.1\n",
      "2023-06-27 21:36:29.167588: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 515.105.1 does not match DSO version 530.30.2 -- cannot find working devices in this configuration\n",
      "2023-06-27 21:36:29.168923: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"stl10\"\n",
    "\n",
    "\n",
    "def prepare_dataset():\n",
    "    unlabeled_batch_size = unlabelled_images // steps_per_epoch\n",
    "    labeled_batch_size = labelled_train_images // steps_per_epoch\n",
    "    batch_size = unlabeled_batch_size + labeled_batch_size\n",
    "\n",
    "    unlabeled_train_dataset = (\n",
    "        tfds.load(\n",
    "            dataset_name, split=\"unlabelled\", as_supervised=True, shuffle_files=True\n",
    "        )\n",
    "        .shuffle(buffer_size=shuffle_buffer)\n",
    "        .batch(unlabeled_batch_size, drop_remainder=True)\n",
    "    )\n",
    "    labeled_train_dataset = (\n",
    "        tfds.load(dataset_name, split=\"train\", as_supervised=True, shuffle_files=True)\n",
    "        .shuffle(buffer_size=shuffle_buffer)\n",
    "        .batch(labeled_batch_size, drop_remainder=True)\n",
    "    )\n",
    "    test_dataset = (\n",
    "        tfds.load(dataset_name, split=\"test\", as_supervised=True)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(buffer_size=AUTOTUNE)\n",
    "    )\n",
    "    train_dataset = tf.data.Dataset.zip(\n",
    "        (unlabeled_train_dataset, labeled_train_dataset)\n",
    "    ).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return batch_size, train_dataset, labeled_train_dataset, test_dataset\n",
    "\n",
    "\n",
    "batch_size, train_dataset, labeled_train_dataset, test_dataset = prepare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgVB6bIK81gn"
   },
   "source": [
    "## Augmentations\n",
    "\n",
    "Other self-supervised techniques like [SimCLR](https://arxiv.org/abs/2002.05709),\n",
    "[BYOL](https://arxiv.org/abs/2006.07733), [SwAV](https://arxiv.org/abs/2006.09882) etc.\n",
    "rely heavily on a well-designed data augmentation pipeline to get the best performance.\n",
    "However, NNCLR is _less_ dependent on complex augmentations as nearest-neighbors already\n",
    "provide richness in sample variations. A few common techniques often included\n",
    "augmentation pipelines are:\n",
    "\n",
    "- Random resized crops\n",
    "- Multiple color distortions\n",
    "- Gaussian blur\n",
    "\n",
    "Since NNCLR is less dependent on complex augmentations, we will only use random\n",
    "crops and random brightness for augmenting the input images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKGPE7-981gn"
   },
   "source": [
    "### Random Resized Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "alhbLMbB81gn"
   },
   "outputs": [],
   "source": [
    "\n",
    "class RandomResizedCrop(layers.Layer):\n",
    "    def __init__(self, scale, ratio):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.log_ratio = (tf.math.log(ratio[0]), tf.math.log(ratio[1]))\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        height = tf.shape(images)[1]\n",
    "        width = tf.shape(images)[2]\n",
    "\n",
    "        random_scales = tf.random.uniform((batch_size,), self.scale[0], self.scale[1])\n",
    "        random_ratios = tf.exp(\n",
    "            tf.random.uniform((batch_size,), self.log_ratio[0], self.log_ratio[1])\n",
    "        )\n",
    "\n",
    "        new_heights = tf.clip_by_value(tf.sqrt(random_scales / random_ratios), 0, 1)\n",
    "        new_widths = tf.clip_by_value(tf.sqrt(random_scales * random_ratios), 0, 1)\n",
    "        height_offsets = tf.random.uniform((batch_size,), 0, 1 - new_heights)\n",
    "        width_offsets = tf.random.uniform((batch_size,), 0, 1 - new_widths)\n",
    "\n",
    "        bounding_boxes = tf.stack(\n",
    "            [\n",
    "                height_offsets,\n",
    "                width_offsets,\n",
    "                height_offsets + new_heights,\n",
    "                width_offsets + new_widths,\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        images = tf.image.crop_and_resize(\n",
    "            images, bounding_boxes, tf.range(batch_size), (height, width)\n",
    "        )\n",
    "        return images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVH9d4n781gn"
   },
   "source": [
    "### Random Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CEvNUpLq81go"
   },
   "outputs": [],
   "source": [
    "\n",
    "class RandomBrightness(layers.Layer):\n",
    "    def __init__(self, brightness):\n",
    "        super().__init__()\n",
    "        self.brightness = brightness\n",
    "\n",
    "    def blend(self, images_1, images_2, ratios):\n",
    "        return tf.clip_by_value(ratios * images_1 + (1.0 - ratios) * images_2, 0, 1)\n",
    "\n",
    "    def random_brightness(self, images):\n",
    "        # random interpolation/extrapolation between the image and darkness\n",
    "        return self.blend(\n",
    "            images,\n",
    "            0,\n",
    "            tf.random.uniform(\n",
    "                (tf.shape(images)[0], 1, 1, 1), 1 - self.brightness, 1 + self.brightness\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def call(self, images):\n",
    "        images = self.random_brightness(images)\n",
    "        return images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNU2QDpI81go"
   },
   "source": [
    "### Prepare augmentation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LzfLR4uL81go"
   },
   "outputs": [],
   "source": [
    "\n",
    "def augmenter(brightness, name, scale):\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=input_shape),\n",
    "            layers.Rescaling(1 / 255),\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            RandomResizedCrop(scale=scale, ratio=(3 / 4, 4 / 3)),\n",
    "            RandomBrightness(brightness=brightness),\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nbfj7p5J81go"
   },
   "source": [
    "### Encoder architecture\n",
    "\n",
    "Using a ResNet-50 as the encoder architecture\n",
    "is standard in the literature. In the original paper, the authors use ResNet-50 as\n",
    "the encoder architecture and spatially average the outputs of ResNet-50. However, keep in\n",
    "mind that more powerful models will not only increase training time but will also\n",
    "require more memory and will limit the maximal batch size you can use. For the purpose of\n",
    "this example, we just use four convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "a8cmm_2D81go"
   },
   "outputs": [],
   "source": [
    "\n",
    "def encoder():\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=input_shape),\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(width, activation=\"relu\"),\n",
    "        ],\n",
    "        name=\"encoder\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjonbP0m81go"
   },
   "source": [
    "## The NNCLR model for contrastive pre-training\n",
    "\n",
    "We train an encoder on unlabeled images with a contrastive loss. A nonlinear projection\n",
    "head is attached to the top of the encoder, as it improves the quality of representations\n",
    "of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YYNdsGFj81go"
   },
   "outputs": [],
   "source": [
    "\n",
    "class NNCLR(keras.Model):\n",
    "    def __init__(\n",
    "        self, temperature, queue_size,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.probe_accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.correlation_accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.probe_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "        self.contrastive_augmenter = augmenter(**contrastive_augmenter)\n",
    "        self.classification_augmenter = augmenter(**classification_augmenter)\n",
    "        self.encoder = encoder()\n",
    "        self.projection_head = keras.Sequential(\n",
    "            [\n",
    "                layers.Input(shape=(width,)),\n",
    "                layers.Dense(width, activation=\"relu\"),\n",
    "                layers.Dense(width),\n",
    "            ],\n",
    "            name=\"projection_head\",\n",
    "        )\n",
    "        self.linear_probe = keras.Sequential(\n",
    "            [layers.Input(shape=(width,)), layers.Dense(10)], name=\"linear_probe\"\n",
    "        )\n",
    "        self.temperature = temperature\n",
    "\n",
    "        feature_dimensions = self.encoder.output_shape[1]\n",
    "        self.feature_queue = tf.Variable(\n",
    "            tf.math.l2_normalize(\n",
    "                tf.random.normal(shape=(queue_size, feature_dimensions)), axis=1\n",
    "            ),\n",
    "            trainable=False,\n",
    "        )\n",
    "\n",
    "    def compile(self, contrastive_optimizer, probe_optimizer, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.contrastive_optimizer = contrastive_optimizer\n",
    "        self.probe_optimizer = probe_optimizer\n",
    "\n",
    "    def nearest_neighbour(self, projections):\n",
    "        support_similarities = tf.matmul(\n",
    "            projections, self.feature_queue, transpose_b=True\n",
    "        )\n",
    "        nn_projections = tf.gather(\n",
    "            self.feature_queue, tf.argmax(support_similarities, axis=1), axis=0\n",
    "        )\n",
    "        return projections + tf.stop_gradient(nn_projections - projections)\n",
    "\n",
    "    def update_contrastive_accuracy(self, features_1, features_2):\n",
    "        features_1 = tf.math.l2_normalize(features_1, axis=1)\n",
    "        features_2 = tf.math.l2_normalize(features_2, axis=1)\n",
    "        similarities = tf.matmul(features_1, features_2, transpose_b=True)\n",
    "\n",
    "        batch_size = tf.shape(features_1)[0]\n",
    "        contrastive_labels = tf.range(batch_size)\n",
    "        self.contrastive_accuracy.update_state(\n",
    "            tf.concat([contrastive_labels, contrastive_labels], axis=0),\n",
    "            tf.concat([similarities, tf.transpose(similarities)], axis=0),\n",
    "        )\n",
    "\n",
    "    def update_correlation_accuracy(self, features_1, features_2):\n",
    "        features_1 = (\n",
    "            features_1 - tf.reduce_mean(features_1, axis=0)\n",
    "        ) / tf.math.reduce_std(features_1, axis=0)\n",
    "        features_2 = (\n",
    "            features_2 - tf.reduce_mean(features_2, axis=0)\n",
    "        ) / tf.math.reduce_std(features_2, axis=0)\n",
    "\n",
    "        batch_size = tf.shape(features_1, out_type=tf.float32)[0]\n",
    "        cross_correlation = (\n",
    "            tf.matmul(features_1, features_2, transpose_a=True) / batch_size\n",
    "        )\n",
    "\n",
    "        feature_dim = tf.shape(features_1)[1]\n",
    "        correlation_labels = tf.range(feature_dim)\n",
    "        self.correlation_accuracy.update_state(\n",
    "            tf.concat([correlation_labels, correlation_labels], axis=0),\n",
    "            tf.concat([cross_correlation, tf.transpose(cross_correlation)], axis=0),\n",
    "        )\n",
    "\n",
    "    def contrastive_loss(self, projections_1, projections_2):\n",
    "        projections_1 = tf.math.l2_normalize(projections_1, axis=1)\n",
    "        projections_2 = tf.math.l2_normalize(projections_2, axis=1)\n",
    "\n",
    "        similarities_1_2_1 = (\n",
    "            tf.matmul(\n",
    "                self.nearest_neighbour(projections_1), projections_2, transpose_b=True\n",
    "            )\n",
    "            / self.temperature\n",
    "        )\n",
    "        similarities_1_2_2 = (\n",
    "            tf.matmul(\n",
    "                projections_2, self.nearest_neighbour(projections_1), transpose_b=True\n",
    "            )\n",
    "            / self.temperature\n",
    "        )\n",
    "\n",
    "        similarities_2_1_1 = (\n",
    "            tf.matmul(\n",
    "                self.nearest_neighbour(projections_2), projections_1, transpose_b=True\n",
    "            )\n",
    "            / self.temperature\n",
    "        )\n",
    "        similarities_2_1_2 = (\n",
    "            tf.matmul(\n",
    "                projections_1, self.nearest_neighbour(projections_2), transpose_b=True\n",
    "            )\n",
    "            / self.temperature\n",
    "        )\n",
    "\n",
    "        batch_size = tf.shape(projections_1)[0]\n",
    "        contrastive_labels = tf.range(batch_size)\n",
    "        loss = keras.losses.sparse_categorical_crossentropy(\n",
    "            tf.concat(\n",
    "                [\n",
    "                    contrastive_labels,\n",
    "                    contrastive_labels,\n",
    "                    contrastive_labels,\n",
    "                    contrastive_labels,\n",
    "                ],\n",
    "                axis=0,\n",
    "            ),\n",
    "            tf.concat(\n",
    "                [\n",
    "                    similarities_1_2_1,\n",
    "                    similarities_1_2_2,\n",
    "                    similarities_2_1_1,\n",
    "                    similarities_2_1_2,\n",
    "                ],\n",
    "                axis=0,\n",
    "            ),\n",
    "            from_logits=True,\n",
    "        )\n",
    "\n",
    "        self.feature_queue.assign(\n",
    "            tf.concat([projections_1, self.feature_queue[:-batch_size]], axis=0)\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        (unlabeled_images, _), (labeled_images, labels) = data\n",
    "        images = tf.concat((unlabeled_images, labeled_images), axis=0)\n",
    "        augmented_images_1 = self.contrastive_augmenter(images)\n",
    "        augmented_images_2 = self.contrastive_augmenter(images)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            features_1 = self.encoder(augmented_images_1)\n",
    "            features_2 = self.encoder(augmented_images_2)\n",
    "            projections_1 = self.projection_head(features_1)\n",
    "            projections_2 = self.projection_head(features_2)\n",
    "            contrastive_loss = self.contrastive_loss(projections_1, projections_2)\n",
    "        gradients = tape.gradient(\n",
    "            contrastive_loss,\n",
    "            self.encoder.trainable_weights + self.projection_head.trainable_weights,\n",
    "        )\n",
    "        self.contrastive_optimizer.apply_gradients(\n",
    "            zip(\n",
    "                gradients,\n",
    "                self.encoder.trainable_weights + self.projection_head.trainable_weights,\n",
    "            )\n",
    "        )\n",
    "        self.update_contrastive_accuracy(features_1, features_2)\n",
    "        self.update_correlation_accuracy(features_1, features_2)\n",
    "        preprocessed_images = self.classification_augmenter(labeled_images)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            features = self.encoder(preprocessed_images)\n",
    "            class_logits = self.linear_probe(features)\n",
    "            probe_loss = self.probe_loss(labels, class_logits)\n",
    "        gradients = tape.gradient(probe_loss, self.linear_probe.trainable_weights)\n",
    "        self.probe_optimizer.apply_gradients(\n",
    "            zip(gradients, self.linear_probe.trainable_weights)\n",
    "        )\n",
    "        self.probe_accuracy.update_state(labels, class_logits)\n",
    "\n",
    "        return {\n",
    "            \"c_loss\": contrastive_loss,\n",
    "            \"c_acc\": self.contrastive_accuracy.result(),\n",
    "            \"r_acc\": self.correlation_accuracy.result(),\n",
    "            \"p_loss\": probe_loss,\n",
    "            \"p_acc\": self.probe_accuracy.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        labeled_images, labels = data\n",
    "\n",
    "        preprocessed_images = self.classification_augmenter(\n",
    "            labeled_images, training=False\n",
    "        )\n",
    "        features = self.encoder(preprocessed_images, training=False)\n",
    "        class_logits = self.linear_probe(features, training=False)\n",
    "        probe_loss = self.probe_loss(labels, class_logits)\n",
    "\n",
    "        self.probe_accuracy.update_state(labels, class_logits)\n",
    "        return {\"p_loss\": probe_loss, \"p_acc\": self.probe_accuracy.result()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbcFtgZW81gp"
   },
   "source": [
    "## Pre-train NNCLR\n",
    "\n",
    "We train the network using a `temperature` of 0.1 as suggested in the paper and\n",
    "a `queue_size` of 10,000 as explained earlier. We use Adam as our contrastive and probe\n",
    "optimizer. For this example we train the model for only 30 epochs but it should be\n",
    "trained for more epochs for better performance.\n",
    "\n",
    "The following two metrics can be used for monitoring the pretraining performance\n",
    "which we also log (taken from\n",
    "[this Keras example](https://keras.io/examples/vision/semisupervised_simclr/#selfsupervised-model-for-contrastive-pretraining)):\n",
    "\n",
    "- Contrastive accuracy: self-supervised metric, the ratio of cases in which the\n",
    "representation of an image is more similar to its differently augmented version's one,\n",
    "than to the representation of any other image in the current batch. Self-supervised\n",
    "metrics can be used for hyperparameter tuning even in the case when there are no labeled\n",
    "examples.\n",
    "- Linear probing accuracy: linear probing is a popular metric to evaluate self-supervised\n",
    "classifiers. It is computed as the accuracy of a logistic regression classifier trained\n",
    "on top of the encoder's features. In our case, this is done by training a single dense\n",
    "layer on top of the frozen encoder. Note that contrary to traditional approach where the\n",
    "classifier is trained after the pretraining phase, in this example we train it during\n",
    "pretraining. This might slightly decrease its accuracy, but that way we can monitor its\n",
    "value during training, which helps with experimentation and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vTsf6Smm81gp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "200/200 [==============================] - ETA: 0s - c_loss: 3.3179 - c_acc: 0.3941 - r_acc: 0.5448 - p_loss: 2.2143 - p_acc: 0.1307"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 21:39:15.978241: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: -9 } dim { size: 96 } dim { size: 96 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"103\" frequency: 3187 num_cores: 24 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 31457280 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: -16 } dim { size: -17 } dim { size: 3 } } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 169s 838ms/step - c_loss: 3.3140 - c_acc: 0.3947 - r_acc: 0.5450 - p_loss: 2.2140 - p_acc: 0.1309 - val_p_loss: 2.1075 - val_p_acc: 0.2362\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 167s 832ms/step - c_loss: 2.1902 - c_acc: 0.6901 - r_acc: 0.6177 - p_loss: 2.0590 - p_acc: 0.2346 - val_p_loss: 1.9993 - val_p_acc: 0.2581\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 167s 832ms/step - c_loss: 1.8882 - c_acc: 0.7708 - r_acc: 0.6184 - p_loss: 1.9560 - p_acc: 0.2648 - val_p_loss: 1.8821 - val_p_acc: 0.3114\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 166s 829ms/step - c_loss: 1.7235 - c_acc: 0.8154 - r_acc: 0.6185 - p_loss: 1.8754 - p_acc: 0.3131 - val_p_loss: 1.8097 - val_p_acc: 0.3428\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 166s 830ms/step - c_loss: 1.6265 - c_acc: 0.8429 - r_acc: 0.6151 - p_loss: 1.8143 - p_acc: 0.3498 - val_p_loss: 1.8066 - val_p_acc: 0.3474\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 167s 831ms/step - c_loss: 1.5632 - c_acc: 0.8568 - r_acc: 0.6144 - p_loss: 1.7718 - p_acc: 0.3629 - val_p_loss: 1.7551 - val_p_acc: 0.3561\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 166s 831ms/step - c_loss: 1.5105 - c_acc: 0.8699 - r_acc: 0.6166 - p_loss: 1.7354 - p_acc: 0.3887 - val_p_loss: 1.7167 - val_p_acc: 0.3568\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 166s 827ms/step - c_loss: 1.4748 - c_acc: 0.8805 - r_acc: 0.6136 - p_loss: 1.7104 - p_acc: 0.3786 - val_p_loss: 1.6728 - val_p_acc: 0.3754\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 166s 828ms/step - c_loss: 1.4561 - c_acc: 0.8861 - r_acc: 0.6121 - p_loss: 1.6860 - p_acc: 0.3895 - val_p_loss: 1.6603 - val_p_acc: 0.3738\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 164s 818ms/step - c_loss: 1.4450 - c_acc: 0.8933 - r_acc: 0.6159 - p_loss: 1.6560 - p_acc: 0.3892 - val_p_loss: 1.6041 - val_p_acc: 0.3816\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 167s 833ms/step - c_loss: 1.4160 - c_acc: 0.8970 - r_acc: 0.6174 - p_loss: 1.6365 - p_acc: 0.3919 - val_p_loss: 1.5734 - val_p_acc: 0.3825\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 167s 833ms/step - c_loss: 1.3925 - c_acc: 0.9032 - r_acc: 0.6176 - p_loss: 1.6215 - p_acc: 0.4012 - val_p_loss: 1.6017 - val_p_acc: 0.3957\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 165s 823ms/step - c_loss: 1.3909 - c_acc: 0.9055 - r_acc: 0.6198 - p_loss: 1.5992 - p_acc: 0.4206 - val_p_loss: 1.5628 - val_p_acc: 0.3874\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 164s 816ms/step - c_loss: 1.3827 - c_acc: 0.9081 - r_acc: 0.6227 - p_loss: 1.5870 - p_acc: 0.4147 - val_p_loss: 1.6097 - val_p_acc: 0.3952\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 164s 820ms/step - c_loss: 1.3632 - c_acc: 0.9113 - r_acc: 0.6238 - p_loss: 1.5610 - p_acc: 0.4036 - val_p_loss: 1.6124 - val_p_acc: 0.4078\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 163s 816ms/step - c_loss: 1.3552 - c_acc: 0.9135 - r_acc: 0.6242 - p_loss: 1.5601 - p_acc: 0.4198 - val_p_loss: 1.5088 - val_p_acc: 0.4083\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 164s 819ms/step - c_loss: 1.3455 - c_acc: 0.9167 - r_acc: 0.6249 - p_loss: 1.5528 - p_acc: 0.4185 - val_p_loss: 1.5780 - val_p_acc: 0.4091\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 163s 817ms/step - c_loss: 1.3500 - c_acc: 0.9144 - r_acc: 0.6256 - p_loss: 1.5477 - p_acc: 0.4237 - val_p_loss: 1.6003 - val_p_acc: 0.4160\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 164s 817ms/step - c_loss: 1.3296 - c_acc: 0.9182 - r_acc: 0.6264 - p_loss: 1.5273 - p_acc: 0.4351 - val_p_loss: 1.5415 - val_p_acc: 0.4207\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 163s 814ms/step - c_loss: 1.3300 - c_acc: 0.9177 - r_acc: 0.6249 - p_loss: 1.5212 - p_acc: 0.4302 - val_p_loss: 1.5633 - val_p_acc: 0.4186\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 163s 815ms/step - c_loss: 1.3256 - c_acc: 0.9213 - r_acc: 0.6250 - p_loss: 1.5135 - p_acc: 0.4180 - val_p_loss: 1.5793 - val_p_acc: 0.4263\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 164s 817ms/step - c_loss: 1.3106 - c_acc: 0.9239 - r_acc: 0.6272 - p_loss: 1.5020 - p_acc: 0.4369 - val_p_loss: 1.5922 - val_p_acc: 0.4342\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 164s 817ms/step - c_loss: 1.3117 - c_acc: 0.9251 - r_acc: 0.6330 - p_loss: 1.4964 - p_acc: 0.4456 - val_p_loss: 1.5799 - val_p_acc: 0.4289\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 164s 818ms/step - c_loss: 1.3105 - c_acc: 0.9252 - r_acc: 0.6328 - p_loss: 1.5017 - p_acc: 0.4415 - val_p_loss: 1.6677 - val_p_acc: 0.4293\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 164s 818ms/step - c_loss: 1.3045 - c_acc: 0.9264 - r_acc: 0.6306 - p_loss: 1.5016 - p_acc: 0.4408 - val_p_loss: 1.5439 - val_p_acc: 0.4384\n"
     ]
    }
   ],
   "source": [
    "model = NNCLR(temperature=temperature, queue_size=queue_size)\n",
    "model.compile(\n",
    "    contrastive_optimizer=keras.optimizers.Adam(),\n",
    "    probe_optimizer=keras.optimizers.Adam(),\n",
    ")\n",
    "pretrain_history = model.fit(\n",
    "    train_dataset, epochs=num_epochs, validation_data=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xE2_pPL881gp"
   },
   "source": [
    "## Evaluate our model\n",
    "\n",
    "A popular way to evaluate a SSL method in computer vision or for that fact any other\n",
    "pre-training method as such is to learn a linear classifier on the frozen features of the\n",
    "trained backbone model and evaluate the classifier on unseen images. Other methods often\n",
    "include fine-tuning on the source dataset or even a target dataset with 5% or 10% labels\n",
    "present. You can use the backbone we just trained for any downstream task such as image\n",
    "classification (like we do here) or segmentation or detection, where the backbone models\n",
    "are usually pre-trained with supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6msroHTr81gp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "199/200 [============================>.] - ETA: 0s - loss: 2.0218 - acc: 0.2501"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 22:45:21.521983: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: -9 } dim { size: 96 } dim { size: 96 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"103\" frequency: 3187 num_cores: 24 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 31457280 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: -16 } dim { size: -17 } dim { size: 3 } } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 8s 40ms/step - loss: 2.0196 - acc: 0.2504 - val_loss: 1.6940 - val_acc: 0.3434\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 1.6188 - acc: 0.3938 - val_loss: 1.5926 - val_acc: 0.4131\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 1.4969 - acc: 0.4496 - val_loss: 1.4588 - val_acc: 0.4629\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 1.4075 - acc: 0.4836 - val_loss: 1.4251 - val_acc: 0.4642\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 1.3645 - acc: 0.4946 - val_loss: 1.3455 - val_acc: 0.4996\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 1.3032 - acc: 0.5168 - val_loss: 1.3168 - val_acc: 0.5161\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 1.2319 - acc: 0.5498 - val_loss: 1.2893 - val_acc: 0.5268\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 1.1898 - acc: 0.5604 - val_loss: 1.4338 - val_acc: 0.4897\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 1.1595 - acc: 0.5876 - val_loss: 1.2700 - val_acc: 0.5401\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 1.1074 - acc: 0.5982 - val_loss: 1.2766 - val_acc: 0.5470\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 1.0626 - acc: 0.6224 - val_loss: 1.2518 - val_acc: 0.5617\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 1.0485 - acc: 0.6204 - val_loss: 1.2884 - val_acc: 0.5396\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 1.0191 - acc: 0.6300 - val_loss: 1.1927 - val_acc: 0.5723\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.9768 - acc: 0.6508 - val_loss: 1.1877 - val_acc: 0.5907\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.9630 - acc: 0.6558 - val_loss: 1.1996 - val_acc: 0.5767\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.9130 - acc: 0.6698 - val_loss: 1.2696 - val_acc: 0.5564\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.8872 - acc: 0.6870 - val_loss: 1.2078 - val_acc: 0.5800\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.8443 - acc: 0.6942 - val_loss: 1.2103 - val_acc: 0.5838\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.8140 - acc: 0.7126 - val_loss: 1.1819 - val_acc: 0.5940\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.8263 - acc: 0.7030 - val_loss: 1.2401 - val_acc: 0.5819\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.7905 - acc: 0.7168 - val_loss: 1.1910 - val_acc: 0.6111\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.7753 - acc: 0.7230 - val_loss: 1.1844 - val_acc: 0.5939\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.7631 - acc: 0.7308 - val_loss: 1.2435 - val_acc: 0.5941\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.7186 - acc: 0.7390 - val_loss: 1.2508 - val_acc: 0.5984\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.7164 - acc: 0.7482 - val_loss: 1.2999 - val_acc: 0.5907\n"
     ]
    }
   ],
   "source": [
    "finetuning_model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=input_shape),\n",
    "        augmenter(**classification_augmenter),\n",
    "        model.encoder,\n",
    "        layers.Dense(10),\n",
    "    ],\n",
    "    name=\"finetuning_model\",\n",
    ")\n",
    "finetuning_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "\n",
    "finetuning_history = finetuning_model.fit(\n",
    "    labeled_train_dataset, epochs=num_epochs, validation_data=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0U222E481gp"
   },
   "source": [
    "Self supervised learning is particularly helpful when you do only have access to very\n",
    "limited labeled training data but you can manage to build a large corpus of unlabeled\n",
    "data as shown by previous methods like [SEER](https://arxiv.org/abs/2103.01988),\n",
    "[SimCLR](https://arxiv.org/abs/2002.05709), [SwAV](https://arxiv.org/abs/2006.09882) and\n",
    "more.\n",
    "\n",
    "You should also take a look at the blog posts for these papers which neatly show that it is\n",
    "possible to achieve good results with few class labels by first pretraining on a large\n",
    "unlabeled dataset and then fine-tuning on a smaller labeled dataset:\n",
    "\n",
    "- [Advancing Self-Supervised and Semi-Supervised Learning with SimCLR](https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html)\n",
    "- [High-performance self-supervised image classification with contrastive clustering](https://ai.facebook.com/blog/high-performance-self-supervised-image-classification-with-contrastive-clustering/)\n",
    "- [Self-supervised learning: The dark matter of intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)\n",
    "\n",
    "You are also advised to check out the [original paper](https://arxiv.org/abs/2104.14548).\n",
    "\n",
    "*Many thanks to [Debidatta Dwibedi](https://twitter.com/debidatta) (Google Research),\n",
    "primary author of the NNCLR paper for his super-insightful reviews for this example.\n",
    "This example also takes inspiration from the [SimCLR Keras Example](https://keras.io/examples/vision/semisupervised_simclr/).*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "nnclr",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
