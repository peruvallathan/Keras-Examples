{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0APRF93Y-MSM"
   },
   "source": [
    "# Learning to tokenize in Vision Transformers\n",
    "\n",
    "**Authors:** [Aritra Roy Gosthipaty](https://twitter.com/ariG23498), [Sayak Paul](https://twitter.com/RisingSayak) (equal contribution)<br>\n",
    "**Date created:** 2021/12/10<br>\n",
    "**Last modified:** 2021/12/15<br>\n",
    "**Description:** Adaptively generating a smaller number of tokens for Vision Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2y2aSEC8-MSO"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Vision Transformers ([Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)) and many\n",
    "other Transformer-based architectures ([Liu et al.](https://arxiv.org/abs/2103.14030),\n",
    "[Yuan et al.](https://arxiv.org/abs/2101.11986), etc.) have shown strong results in\n",
    "image recognition. The following provides a brief overview of the components involved in the\n",
    "Vision Transformer architecture for image classification:\n",
    "\n",
    "* Extract small patches from input images.\n",
    "* Linearly project those patches.\n",
    "* Add positional embeddings to these linear projections.\n",
    "* Run these projections through a series of Transformer ([Vaswani et al.](https://arxiv.org/abs/1706.03762))\n",
    "blocks.\n",
    "* Finally, take the representation from the final Transformer block and add a\n",
    "classification head.\n",
    "\n",
    "If we take 224x224 images and extract 16x16 patches, we get a total of 196 patches (also\n",
    "called tokens) for each image. The number of patches increases as we increase the\n",
    "resolution, leading to higher memory footprint. Could we use a reduced\n",
    "number of patches without having to compromise performance?\n",
    "Ryoo et al. investigate this question in\n",
    "[TokenLearner: Adaptive Space-Time Tokenization for Videos](https://openreview.net/forum?id=z-l1kpDXs88).\n",
    "They introduce a novel module called **TokenLearner** that can help reduce the number\n",
    "of patches used by a Vision Transformer (ViT) in an adaptive manner. With TokenLearner\n",
    "incorporated in the standard ViT architecture, they are able to reduce the amount of\n",
    "compute (measured in FLOPS) used by the model.\n",
    "\n",
    "In this example, we implement the TokenLearner module and demonstrate its\n",
    "performance with a mini ViT and the CIFAR-10 dataset. We make use of the following\n",
    "references:\n",
    "\n",
    "* [Official TokenLearner code](https://github.com/google-research/scenic/blob/main/scenic/projects/token_learner/model.py)\n",
    "* [Image Classification with ViTs on keras.io](https://keras.io/examples/vision/image_classification_with_vision_transformer/)\n",
    "* [TokenLearner slides from NeurIPS 2021](https://nips.cc/media/neurips-2021/Slides/26578.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlx_T4w0-MSP"
   },
   "source": [
    "## Setup\n",
    "\n",
    "We need to install TensorFlow Addons to run this example. To install it, execute the\n",
    "following:\n",
    "\n",
    "```shell\n",
    "pip install tensorflow-addons\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5qnGzJg-MSP"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fzGZBnbU-MSP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-28 14:38:48.231021: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.9.3 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNMZbzwF-MSQ"
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Please feel free to change the hyperparameters and check your results. The best way to\n",
    "develop intuition about the architecture is to experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U4oaBa9T-MSQ"
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "BATCH_SIZE = 256\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 20\n",
    "\n",
    "# AUGMENTATION\n",
    "IMAGE_SIZE = 48  # We will resize input images to this size.\n",
    "PATCH_SIZE = 6  # Size of the patches to be extracted from the input images.\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "# ViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 4\n",
    "MLP_UNITS = [\n",
    "    PROJECTION_DIM * 2,\n",
    "    PROJECTION_DIM,\n",
    "]\n",
    "\n",
    "# TOKENLEARNER\n",
    "NUM_TOKENS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynjTq66V-MSQ"
   },
   "source": [
    "## Load and prepare the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-cchg_UO-MSQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 40000\n",
      "Validation samples: 10000\n",
      "Testing samples: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-28 14:38:49.444057: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2023-06-28 14:38:49.444078: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: zlabshwa01\n",
      "2023-06-28 14:38:49.444081: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: zlabshwa01\n",
      "2023-06-28 14:38:49.444150: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 530.30.2\n",
      "2023-06-28 14:38:49.444160: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.105.1\n",
      "2023-06-28 14:38:49.444162: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 515.105.1 does not match DSO version 530.30.2 -- cannot find working devices in this configuration\n",
      "2023-06-28 14:38:49.444344: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR-10 dataset.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "(x_train, y_train), (x_val, y_val) = (\n",
    "    (x_train[:40000], y_train[:40000]),\n",
    "    (x_train[40000:], y_train[40000:]),\n",
    ")\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Validation samples: {len(x_val)}\")\n",
    "print(f\"Testing samples: {len(x_test)}\")\n",
    "\n",
    "# Convert to tf.data.Dataset objects.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(BATCH_SIZE * 100).batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4QJL6dS-MSQ"
   },
   "source": [
    "## Data augmentation\n",
    "\n",
    "The augmentation pipeline consists of:\n",
    "\n",
    "- Rescaling\n",
    "- Resizing\n",
    "- Random cropping (fixed-sized or random sized)\n",
    "- Random horizontal flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TGi6oFXK-MSR"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Rescaling(1 / 255.0),\n",
    "        layers.Resizing(INPUT_SHAPE[0] + 20, INPUT_SHAPE[0] + 20),\n",
    "        layers.RandomCrop(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjnwbwWv-MSR"
   },
   "source": [
    "Note that image data augmentation layers do not apply data transformations at inference time.\n",
    "This means that when these layers are called with `training=False` they behave differently. Refer\n",
    "[to the documentation](https://keras.io/api/layers/preprocessing_layers/image_augmentation/) for more\n",
    "details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ld06kEd7-MSR"
   },
   "source": [
    "## Positional embedding module\n",
    "\n",
    "A [Transformer](https://arxiv.org/abs/1706.03762) architecture consists of **multi-head\n",
    "self attention** layers and **fully-connected feed forward** networks (MLP) as the main\n",
    "components. Both these components are _permutation invariant_: they're not aware of\n",
    "feature order.\n",
    "\n",
    "To overcome this problem we inject tokens with positional information. The\n",
    "`position_embedding` function adds this positional information to the linearly projected\n",
    "tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jracVvxI-MSR"
   },
   "outputs": [],
   "source": [
    "\n",
    "def position_embedding(\n",
    "    projected_patches, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM\n",
    "):\n",
    "    # Build the positions.\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "\n",
    "    # Encode the positions with an Embedding layer.\n",
    "    encoded_positions = layers.Embedding(\n",
    "        input_dim=num_patches, output_dim=projection_dim\n",
    "    )(positions)\n",
    "\n",
    "    # Add encoded positions to the projected patches.\n",
    "    return projected_patches + encoded_positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlQ404_8-MSR"
   },
   "source": [
    "## MLP block for Transformer\n",
    "\n",
    "This serves as the Fully Connected Feed Forward block for our Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wPw2FZRl-MSR"
   },
   "outputs": [],
   "source": [
    "\n",
    "def mlp(x, dropout_rate, hidden_units):\n",
    "    # Iterate over the hidden units and\n",
    "    # add Dense => Dropout.\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "draDPf_3-MSR"
   },
   "source": [
    "## TokenLearner module\n",
    "\n",
    "The following figure presents a pictorial overview of the module\n",
    "([source](https://ai.googleblog.com/2021/12/improving-vision-transformer-efficiency.html)).\n",
    "\n",
    "![TokenLearner module GIF](https://blogger.googleusercontent.com/img/a/AVvXsEiylT3_nmd9-tzTnz3g3Vb4eTn-L5sOwtGJOad6t2we7FsjXSpbLDpuPrlInAhtE5hGCA_PfYTJtrIOKfLYLYGcYXVh1Ksfh_C1ZC-C8gw6GKtvrQesKoMrEA_LU_Gd5srl5-3iZDgJc1iyCELoXtfuIXKJ2ADDHOBaUjhU8lXTVdr2E7bCVaFgVHHkmA=w640-h208)\n",
    "\n",
    "The TokenLearner module takes as input an image-shaped tensor. It then passes it through\n",
    "multiple single-channel convolutional layers extracting different spatial attention maps\n",
    "focusing on different parts of the input. These attention maps are then element-wise\n",
    "multiplied to the input and result is aggregated with pooling. This pooled output can be\n",
    "trated as a summary of the input and has much lesser number of patches (8, for example)\n",
    "than the original one (196, for example).\n",
    "\n",
    "Using multiple convolution layers helps with expressivity. Imposing a form of spatial\n",
    "attention helps retain relevant information from the inputs. Both of these components are\n",
    "crucial to make TokenLearner work, especially when we are significantly reducing the number of patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UeISOKe6-MSS"
   },
   "outputs": [],
   "source": [
    "\n",
    "def token_learner(inputs, number_of_tokens=NUM_TOKENS):\n",
    "    # Layer normalize the inputs.\n",
    "    x = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(inputs)  # (B, H, W, C)\n",
    "\n",
    "    # Applying Conv2D => Reshape => Permute\n",
    "    # The reshape and permute is done to help with the next steps of\n",
    "    # multiplication and Global Average Pooling.\n",
    "    attention_maps = keras.Sequential(\n",
    "        [\n",
    "            # 3 layers of conv with gelu activation as suggested\n",
    "            # in the paper.\n",
    "            layers.Conv2D(\n",
    "                filters=number_of_tokens,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=tf.nn.gelu,\n",
    "                padding=\"same\",\n",
    "                use_bias=False,\n",
    "            ),\n",
    "            layers.Conv2D(\n",
    "                filters=number_of_tokens,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=tf.nn.gelu,\n",
    "                padding=\"same\",\n",
    "                use_bias=False,\n",
    "            ),\n",
    "            layers.Conv2D(\n",
    "                filters=number_of_tokens,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=tf.nn.gelu,\n",
    "                padding=\"same\",\n",
    "                use_bias=False,\n",
    "            ),\n",
    "            # This conv layer will generate the attention maps\n",
    "            layers.Conv2D(\n",
    "                filters=number_of_tokens,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=\"sigmoid\",  # Note sigmoid for [0, 1] output\n",
    "                padding=\"same\",\n",
    "                use_bias=False,\n",
    "            ),\n",
    "            # Reshape and Permute\n",
    "            layers.Reshape((-1, number_of_tokens)),  # (B, H*W, num_of_tokens)\n",
    "            layers.Permute((2, 1)),\n",
    "        ]\n",
    "    )(\n",
    "        x\n",
    "    )  # (B, num_of_tokens, H*W)\n",
    "\n",
    "    # Reshape the input to align it with the output of the conv block.\n",
    "    num_filters = inputs.shape[-1]\n",
    "    inputs = layers.Reshape((1, -1, num_filters))(inputs)  # inputs == (B, 1, H*W, C)\n",
    "\n",
    "    # Element-Wise multiplication of the attention maps and the inputs\n",
    "    attended_inputs = (\n",
    "        attention_maps[..., tf.newaxis] * inputs\n",
    "    )  # (B, num_tokens, H*W, C)\n",
    "\n",
    "    # Global average pooling the element wise multiplication result.\n",
    "    outputs = tf.reduce_mean(attended_inputs, axis=2)  # (B, num_tokens, C)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QlM83Di-MSS"
   },
   "source": [
    "## Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TaTicRma-MSS"
   },
   "outputs": [],
   "source": [
    "\n",
    "def transformer(encoded_patches):\n",
    "    # Layer normalization 1.\n",
    "    x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n",
    "\n",
    "    # Multi Head Self Attention layer 1.\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
    "    )(x1, x1)\n",
    "\n",
    "    # Skip connection 1.\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "    # Layer normalization 2.\n",
    "    x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)\n",
    "\n",
    "    # MLP layer 1.\n",
    "    x4 = mlp(x3, hidden_units=MLP_UNITS, dropout_rate=0.1)\n",
    "\n",
    "    # Skip connection 2.\n",
    "    encoded_patches = layers.Add()([x4, x2])\n",
    "    return encoded_patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn--j6Dj-MSS"
   },
   "source": [
    "## ViT model with the TokenLearner module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "C24A8Hyd-MSS"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_vit_classifier(use_token_learner=True, token_learner_units=NUM_TOKENS):\n",
    "    inputs = layers.Input(shape=INPUT_SHAPE)  # (B, H, W, C)\n",
    "\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "\n",
    "    # Create patches and project the pathces.\n",
    "    projected_patches = layers.Conv2D(\n",
    "        filters=PROJECTION_DIM,\n",
    "        kernel_size=(PATCH_SIZE, PATCH_SIZE),\n",
    "        strides=(PATCH_SIZE, PATCH_SIZE),\n",
    "        padding=\"VALID\",\n",
    "    )(augmented)\n",
    "    _, h, w, c = projected_patches.shape\n",
    "    projected_patches = layers.Reshape((h * w, c))(\n",
    "        projected_patches\n",
    "    )  # (B, number_patches, projection_dim)\n",
    "\n",
    "    # Add positional embeddings to the projected patches.\n",
    "    encoded_patches = position_embedding(\n",
    "        projected_patches\n",
    "    )  # (B, number_patches, projection_dim)\n",
    "    encoded_patches = layers.Dropout(0.1)(encoded_patches)\n",
    "\n",
    "    # Iterate over the number of layers and stack up blocks of\n",
    "    # Transformer.\n",
    "    for i in range(NUM_LAYERS):\n",
    "        # Add a Transformer block.\n",
    "        encoded_patches = transformer(encoded_patches)\n",
    "\n",
    "        # Add TokenLearner layer in the middle of the\n",
    "        # architecture. The paper suggests that anywhere\n",
    "        # between 1/2 or 3/4 will work well.\n",
    "        if use_token_learner and i == NUM_LAYERS // 2:\n",
    "            _, hh, c = encoded_patches.shape\n",
    "            h = int(math.sqrt(hh))\n",
    "            encoded_patches = layers.Reshape((h, h, c))(\n",
    "                encoded_patches\n",
    "            )  # (B, h, h, projection_dim)\n",
    "            encoded_patches = token_learner(\n",
    "                encoded_patches, token_learner_units\n",
    "            )  # (B, num_tokens, c)\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujY5f_ph-MSS"
   },
   "source": [
    "As shown in the [TokenLearner paper](https://openreview.net/forum?id=z-l1kpDXs88), it is\n",
    "almost always advantageous to include the TokenLearner module in the middle of the\n",
    "network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMTb6LbO-MSS"
   },
   "source": [
    "## Training utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dKqG4u-G-MSS"
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment(model):\n",
    "    # Initialize the AdamW optimizer.\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Define callbacks\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(\n",
    "        train_ds,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(test_ds)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGPfZL1y-MSS"
   },
   "source": [
    "## Train and evaluate a ViT with TokenLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9vTW2ETZ-MSS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "157/157 [==============================] - 85s 528ms/step - loss: 2.3034 - accuracy: 0.1329 - top-5-accuracy: 0.5758 - val_loss: 2.0633 - val_accuracy: 0.2126 - val_top-5-accuracy: 0.7609\n",
      "Epoch 2/20\n",
      "157/157 [==============================] - 83s 529ms/step - loss: 1.9500 - accuracy: 0.2581 - top-5-accuracy: 0.8047 - val_loss: 1.9294 - val_accuracy: 0.2805 - val_top-5-accuracy: 0.8124\n",
      "Epoch 3/20\n",
      "157/157 [==============================] - 83s 526ms/step - loss: 1.7702 - accuracy: 0.3338 - top-5-accuracy: 0.8604 - val_loss: 1.7943 - val_accuracy: 0.3295 - val_top-5-accuracy: 0.8592\n",
      "Epoch 4/20\n",
      "157/157 [==============================] - 82s 522ms/step - loss: 1.6637 - accuracy: 0.3785 - top-5-accuracy: 0.8852 - val_loss: 1.6459 - val_accuracy: 0.3800 - val_top-5-accuracy: 0.9000\n",
      "Epoch 5/20\n",
      "157/157 [==============================] - 82s 524ms/step - loss: 1.5793 - accuracy: 0.4146 - top-5-accuracy: 0.9003 - val_loss: 1.5939 - val_accuracy: 0.4177 - val_top-5-accuracy: 0.8989\n",
      "Epoch 6/20\n",
      "157/157 [==============================] - 82s 520ms/step - loss: 1.5114 - accuracy: 0.4401 - top-5-accuracy: 0.9116 - val_loss: 1.4864 - val_accuracy: 0.4585 - val_top-5-accuracy: 0.9149\n",
      "Epoch 7/20\n",
      "157/157 [==============================] - 82s 522ms/step - loss: 1.4682 - accuracy: 0.4615 - top-5-accuracy: 0.9184 - val_loss: 1.4725 - val_accuracy: 0.4549 - val_top-5-accuracy: 0.9233\n",
      "Epoch 8/20\n",
      "157/157 [==============================] - 82s 523ms/step - loss: 1.4276 - accuracy: 0.4768 - top-5-accuracy: 0.9250 - val_loss: 1.4275 - val_accuracy: 0.4852 - val_top-5-accuracy: 0.9229\n",
      "Epoch 9/20\n",
      "157/157 [==============================] - 82s 523ms/step - loss: 1.3989 - accuracy: 0.4890 - top-5-accuracy: 0.9306 - val_loss: 1.4058 - val_accuracy: 0.4858 - val_top-5-accuracy: 0.9277\n",
      "Epoch 10/20\n",
      "157/157 [==============================] - 82s 523ms/step - loss: 1.3744 - accuracy: 0.4973 - top-5-accuracy: 0.9334 - val_loss: 1.3391 - val_accuracy: 0.5165 - val_top-5-accuracy: 0.9339\n",
      "Epoch 11/20\n",
      "157/157 [==============================] - 82s 522ms/step - loss: 1.3431 - accuracy: 0.5098 - top-5-accuracy: 0.9367 - val_loss: 1.4100 - val_accuracy: 0.4879 - val_top-5-accuracy: 0.9305\n",
      "Epoch 12/20\n",
      "157/157 [==============================] - 82s 524ms/step - loss: 1.3297 - accuracy: 0.5178 - top-5-accuracy: 0.9395 - val_loss: 1.3036 - val_accuracy: 0.5255 - val_top-5-accuracy: 0.9447\n",
      "Epoch 13/20\n",
      "157/157 [==============================] - 82s 524ms/step - loss: 1.2969 - accuracy: 0.5280 - top-5-accuracy: 0.9435 - val_loss: 1.3445 - val_accuracy: 0.5167 - val_top-5-accuracy: 0.9365\n",
      "Epoch 14/20\n",
      "157/157 [==============================] - 82s 521ms/step - loss: 1.2832 - accuracy: 0.5334 - top-5-accuracy: 0.9445 - val_loss: 1.3139 - val_accuracy: 0.5239 - val_top-5-accuracy: 0.9425\n",
      "Epoch 15/20\n",
      "157/157 [==============================] - 82s 520ms/step - loss: 1.2771 - accuracy: 0.5365 - top-5-accuracy: 0.9445 - val_loss: 1.3478 - val_accuracy: 0.5127 - val_top-5-accuracy: 0.9348\n",
      "Epoch 16/20\n",
      "157/157 [==============================] - 82s 521ms/step - loss: 1.2544 - accuracy: 0.5445 - top-5-accuracy: 0.9483 - val_loss: 1.2817 - val_accuracy: 0.5350 - val_top-5-accuracy: 0.9457\n",
      "Epoch 17/20\n",
      "157/157 [==============================] - 83s 527ms/step - loss: 1.2301 - accuracy: 0.5543 - top-5-accuracy: 0.9497 - val_loss: 1.2154 - val_accuracy: 0.5578 - val_top-5-accuracy: 0.9494\n",
      "Epoch 18/20\n",
      "157/157 [==============================] - 83s 529ms/step - loss: 1.2100 - accuracy: 0.5620 - top-5-accuracy: 0.9531 - val_loss: 1.2236 - val_accuracy: 0.5625 - val_top-5-accuracy: 0.9481\n",
      "Epoch 19/20\n",
      "157/157 [==============================] - 83s 529ms/step - loss: 1.2095 - accuracy: 0.5633 - top-5-accuracy: 0.9521 - val_loss: 1.2486 - val_accuracy: 0.5507 - val_top-5-accuracy: 0.9507\n",
      "Epoch 20/20\n",
      "157/157 [==============================] - 83s 528ms/step - loss: 1.1817 - accuracy: 0.5719 - top-5-accuracy: 0.9574 - val_loss: 1.1859 - val_accuracy: 0.5711 - val_top-5-accuracy: 0.9515\n",
      "40/40 [==============================] - 8s 194ms/step - loss: 1.1917 - accuracy: 0.5710 - top-5-accuracy: 0.9546\n",
      "Test accuracy: 57.1%\n",
      "Test top 5 accuracy: 95.46%\n"
     ]
    }
   ],
   "source": [
    "vit_token_learner = create_vit_classifier()\n",
    "run_experiment(vit_token_learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGAOxpKk-MST"
   },
   "source": [
    "## Results\n",
    "\n",
    "We experimented with and without the TokenLearner inside the mini ViT we implemented\n",
    "(with the same hyperparameters presented in this example). Here are our results:\n",
    "\n",
    "| **TokenLearner** | **# tokens in<br> TokenLearner** | **Top-1 Acc<br>(Averaged across 5 runs)** | **GFLOPs** | **TensorBoard** |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| N | - | 56.112% | 0.0184 | [Link](https://tensorboard.dev/experiment/vkCwM49dQZ2RiK0ZT4mj7w/) |\n",
    "| Y | 8 | **56.55%** | **0.0153** | [Link](https://tensorboard.dev/experiment/vkCwM49dQZ2RiK0ZT4mj7w/) |\n",
    "| N | - | 56.37% | 0.0184 | [Link](https://tensorboard.dev/experiment/hdyJ4wznQROwqZTgbtmztQ/) |\n",
    "| Y | 4 | **56.4980%** | **0.0147** | [Link](https://tensorboard.dev/experiment/hdyJ4wznQROwqZTgbtmztQ/) |\n",
    "| N | - (# Transformer layers: 8) | 55.36% | 0.0359 | [Link](https://tensorboard.dev/experiment/sepBK5zNSaOtdCeEG6SV9w/) |\n",
    "\n",
    "TokenLearner is able to consistently outperform our mini ViT without the module. It is\n",
    "also interesting to notice that it was also able to outperform a deeper version of our\n",
    "mini ViT (with 8 layers). The authors also report similar observations in the paper and\n",
    "they attribute this to the adaptiveness of TokenLearner.\n",
    "\n",
    "One should also note that the FLOPs count **decreases** considerably with the addition of\n",
    "the TokenLearner module. With less FLOPs count the TokenLearner module is able to\n",
    "deliver better results. This aligns very well with the authors' findings.\n",
    "\n",
    "Additionally, the authors [introduced](https://github.com/google-research/scenic/blob/main/scenic/projects/token_learner/model.py#L104)\n",
    "a newer version of the TokenLearner for smaller training data regimes. Quoting the authors:\n",
    "\n",
    "> Instead of using 4 conv. layers with small channels to implement spatial attention,\n",
    "  this version uses 2 grouped conv. layers with more channels. It also uses softmax\n",
    "  instead of sigmoid. We confirmed that this version works better when having limited\n",
    "  training data, such as training with ImageNet1K from scratch.\n",
    "\n",
    "We experimented with this module and in the following table we summarize the results:\n",
    "\n",
    "| **# Groups** | **# Tokens** | **Top-1 Acc** | **GFLOPs** | **TensorBoard** |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| 4 | 4 | 54.638% | 0.0149 | [Link](https://tensorboard.dev/experiment/KmfkGqAGQjikEw85phySmw/) |\n",
    "| 8 | 8 | 54.898% | 0.0146 | [Link](https://tensorboard.dev/experiment/0PpgYOq9RFWV9njX6NJQ2w/) |\n",
    "| 4 | 8 | 55.196% | 0.0149 | [Link](https://tensorboard.dev/experiment/WUkrHbZASdu3zrfmY4ETZg/) |\n",
    "\n",
    "Please note that we used the same hyperparameters presented in this example. Our\n",
    "implementation is available\n",
    "[in this notebook](https://github.com/ariG23498/TokenLearner/blob/master/TokenLearner-V1.1.ipynb).\n",
    "We acknowledge that the results with this new TokenLearner module are slightly off\n",
    "than expected and this might mitigate with hyperparameter tuning.\n",
    "\n",
    "*Note*: To compute the FLOPs of our models we used\n",
    "[this utility](https://github.com/AdityaKane2001/regnety/blob/main/regnety/utils/model_utils.py#L27)\n",
    "from [this repository](https://github.com/AdityaKane2001/regnety)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ImCetad-MST"
   },
   "source": [
    "## Number of parameters\n",
    "\n",
    "You may have noticed that adding the TokenLearner module increases the number of\n",
    "parameters of the base network. But that does not mean it is less efficient as shown by\n",
    "[Dehghani et al.](https://arxiv.org/abs/2110.12894). Similar findings were reported\n",
    "by [Bello et al.](https://arxiv.org/abs/2103.07579) as well. The TokenLearner module\n",
    "helps reducing the FLOPS in the overall network thereby helping to reduce the memory\n",
    "footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cU55RmGI-MST"
   },
   "source": [
    "## Final notes\n",
    "\n",
    "* TokenFuser: The authors of the paper also propose another module named TokenFuser. This\n",
    "module helps in remapping the representation of the TokenLearner output back to its\n",
    "original spatial resolution. To reuse the TokenLearner in the ViT architecture, the\n",
    "TokenFuser is a must. We first learn the tokens from the TokenLearner, build a\n",
    "representation of the tokens from a Transformer layer and then remap the representation\n",
    "into the original spatial resolution, so that it can again be consumed by a TokenLearner.\n",
    "Note here that you can only use the TokenLearner module once in entire ViT model if not\n",
    "paired with the TokenFuser.\n",
    "* Use of these modules for video: The authors also suggest that TokenFuser goes really\n",
    "well with Vision Transformers for Videos ([Arnab et al.](https://arxiv.org/abs/2103.15691)).\n",
    "\n",
    "We are grateful to [JarvisLabs](https://jarvislabs.ai/) and\n",
    "[Google Developers Experts](https://developers.google.com/programs/experts/)\n",
    "program for helping with GPU credits. Also, we are thankful to Michael Ryoo (first\n",
    "author of TokenLearner) for fruitful discussions.\n",
    "\n",
    "| Trained Model | Demo |\n",
    "| :--: | :--: |\n",
    "| [![Generic badge](https://img.shields.io/badge/🤗%20Model-TokenLearner-black.svg)](https://huggingface.co/keras-io/learning_to_tokenize_in_ViT) | [![Generic badge](https://img.shields.io/badge/🤗%20Spaces-TokenLearner-black.svg)](https://huggingface.co/spaces/keras-io/token_learner) |"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "token_learner",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
