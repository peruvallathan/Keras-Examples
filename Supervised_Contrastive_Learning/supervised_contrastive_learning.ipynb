{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoS3ysZ_99KF"
   },
   "source": [
    "# Supervised Contrastive Learning\n",
    "\n",
    "**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n",
    "**Date created:** 2020/11/30<br>\n",
    "**Last modified:** 2020/11/30<br>\n",
    "**Description:** Using supervised contrastive learning for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HY8YbBMQ99KH"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "[Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n",
    "(Prannay Khosla et al.) is a training methodology that outperforms\n",
    "supervised training with crossentropy on classification tasks.\n",
    "\n",
    "Essentially, training an image classification model with Supervised Contrastive\n",
    "Learning is performed in two phases:\n",
    "\n",
    "1. Training an encoder to learn to produce vector representations of input images such\n",
    "that representations of images in the same class will be more similar compared to\n",
    "representations of images in different classes.\n",
    "2. Training a classifier on top of the frozen encoder.\n",
    "\n",
    "Note that this example requires [TensorFlow Addons](https://www.tensorflow.org/addons), which you can install using the following command:\n",
    "\n",
    "```python\n",
    "pip install tensorflow-addons\n",
    "```\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "itqwCLaL99KH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-28 12:15:36.258783: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.9.3 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yY5G8bc99KI"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "brU-hXVD99KI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# Load the train and test data splits\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Display shapes of train and test datasets\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkkO9Y0m99KI"
   },
   "source": [
    "## Using image data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wfDRse6799KI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-28 12:15:37.366047: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2023-06-28 12:15:37.366071: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: zlabshwa01\n",
      "2023-06-28 12:15:37.366074: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: zlabshwa01\n",
      "2023-06-28 12:15:37.366141: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 530.30.2\n",
      "2023-06-28 12:15:37.366153: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.105.1\n",
      "2023-06-28 12:15:37.366156: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 515.105.1 does not match DSO version 530.30.2 -- cannot find working devices in this configuration\n",
      "2023-06-28 12:15:37.366500: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.02),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setting the state of the normalization layer.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7GNrSEz99KJ"
   },
   "source": [
    "## Build the encoder model\n",
    "\n",
    "The encoder model takes the image as input and turns it into a 2048-dimensional\n",
    "feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XbQJjosY99KJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar10-encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 32, 32, 3)         7         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,564,807\n",
      "Trainable params: 23,519,360\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_encoder():\n",
    "    resnet = keras.applications.ResNet50V2(\n",
    "        include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\"\n",
    "    )\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    augmented = data_augmentation(inputs)\n",
    "    outputs = resnet(augmented)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n",
    "    return model\n",
    "\n",
    "\n",
    "encoder = create_encoder()\n",
    "encoder.summary()\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 265\n",
    "hidden_units = 512\n",
    "projection_units = 128\n",
    "num_epochs = 50\n",
    "dropout_rate = 0.5\n",
    "temperature = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al217xaJ99KJ"
   },
   "source": [
    "## Build the classification model\n",
    "\n",
    "The classification model adds a fully-connected layer on top of the encoder,\n",
    "plus a softmax layer with the target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xxVx7U5e99KJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_classifier(encoder, trainable=True):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXLK8rBy99KJ"
   },
   "source": [
    "## Experiment 1: Train the baseline classification model\n",
    "\n",
    "In this experiment, a baseline classifier is trained as usual, i.e., the\n",
    "encoder and the classifier parts are trained together as a single model\n",
    "to minimize the crossentropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nay3Bqxd99KJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar10-classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " cifar10-encoder (Functional  (None, 2048)             23564807  \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,619,025\n",
      "Trainable params: 24,573,578\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "189/189 [==============================] - 64s 327ms/step - loss: 1.8616 - sparse_categorical_accuracy: 0.3243\n",
      "Epoch 2/50\n",
      "189/189 [==============================] - 62s 326ms/step - loss: 1.4268 - sparse_categorical_accuracy: 0.4901\n",
      "Epoch 3/50\n",
      "189/189 [==============================] - 62s 326ms/step - loss: 1.3345 - sparse_categorical_accuracy: 0.5240\n",
      "Epoch 4/50\n",
      "189/189 [==============================] - 62s 328ms/step - loss: 1.1413 - sparse_categorical_accuracy: 0.6013\n",
      "Epoch 5/50\n",
      "189/189 [==============================] - 62s 327ms/step - loss: 1.0230 - sparse_categorical_accuracy: 0.6456\n",
      "Epoch 6/50\n",
      "189/189 [==============================] - 62s 326ms/step - loss: 0.9368 - sparse_categorical_accuracy: 0.6775\n",
      "Epoch 7/50\n",
      "189/189 [==============================] - 62s 326ms/step - loss: 0.8745 - sparse_categorical_accuracy: 0.7004\n",
      "Epoch 8/50\n",
      "189/189 [==============================] - 62s 327ms/step - loss: 0.8118 - sparse_categorical_accuracy: 0.7259\n",
      "Epoch 9/50\n",
      "189/189 [==============================] - 62s 327ms/step - loss: 0.7688 - sparse_categorical_accuracy: 0.7399\n",
      "Epoch 10/50\n",
      "189/189 [==============================] - 62s 326ms/step - loss: 0.7197 - sparse_categorical_accuracy: 0.7575\n",
      "Epoch 11/50\n",
      "189/189 [==============================] - 62s 326ms/step - loss: 0.6812 - sparse_categorical_accuracy: 0.7711\n",
      "Epoch 12/50\n",
      "189/189 [==============================] - 62s 327ms/step - loss: 0.6718 - sparse_categorical_accuracy: 0.7717\n",
      "Epoch 13/50\n",
      "189/189 [==============================] - 62s 327ms/step - loss: 0.7651 - sparse_categorical_accuracy: 0.7369\n",
      "Epoch 14/50\n",
      "189/189 [==============================] - 62s 326ms/step - loss: 0.6151 - sparse_categorical_accuracy: 0.7940\n",
      "Epoch 15/50\n",
      "189/189 [==============================] - 61s 325ms/step - loss: 0.5471 - sparse_categorical_accuracy: 0.8163\n",
      "Epoch 16/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.4998 - sparse_categorical_accuracy: 0.8321\n",
      "Epoch 17/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 0.4694 - sparse_categorical_accuracy: 0.8418\n",
      "Epoch 18/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 0.4340 - sparse_categorical_accuracy: 0.8521\n",
      "Epoch 19/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.4140 - sparse_categorical_accuracy: 0.8611\n",
      "Epoch 20/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.3852 - sparse_categorical_accuracy: 0.8712\n",
      "Epoch 21/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.3625 - sparse_categorical_accuracy: 0.8792\n",
      "Epoch 22/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.3452 - sparse_categorical_accuracy: 0.8836\n",
      "Epoch 23/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.6849 - sparse_categorical_accuracy: 0.7686\n",
      "Epoch 24/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.4164 - sparse_categorical_accuracy: 0.8593\n",
      "Epoch 25/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.3384 - sparse_categorical_accuracy: 0.8852\n",
      "Epoch 26/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.3093 - sparse_categorical_accuracy: 0.8964\n",
      "Epoch 27/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.2865 - sparse_categorical_accuracy: 0.9046\n",
      "Epoch 28/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 0.2638 - sparse_categorical_accuracy: 0.9104\n",
      "Epoch 29/50\n",
      "189/189 [==============================] - 61s 324ms/step - loss: 0.2450 - sparse_categorical_accuracy: 0.9175\n",
      "Epoch 30/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.2368 - sparse_categorical_accuracy: 0.9190\n",
      "Epoch 31/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 0.2256 - sparse_categorical_accuracy: 0.9248\n",
      "Epoch 32/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.2072 - sparse_categorical_accuracy: 0.9306\n",
      "Epoch 33/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.2467 - sparse_categorical_accuracy: 0.9206\n",
      "Epoch 34/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 0.2423 - sparse_categorical_accuracy: 0.9208\n",
      "Epoch 35/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.2245 - sparse_categorical_accuracy: 0.9240\n",
      "Epoch 36/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.1779 - sparse_categorical_accuracy: 0.9406\n",
      "Epoch 37/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.1654 - sparse_categorical_accuracy: 0.9441\n",
      "Epoch 38/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.1614 - sparse_categorical_accuracy: 0.9454\n",
      "Epoch 39/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.1472 - sparse_categorical_accuracy: 0.9514\n",
      "Epoch 40/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.1457 - sparse_categorical_accuracy: 0.9503\n",
      "Epoch 41/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.1373 - sparse_categorical_accuracy: 0.9551\n",
      "Epoch 42/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.1352 - sparse_categorical_accuracy: 0.9547\n",
      "Epoch 43/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.1306 - sparse_categorical_accuracy: 0.9565\n",
      "Epoch 44/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.1248 - sparse_categorical_accuracy: 0.9585\n",
      "Epoch 45/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.1235 - sparse_categorical_accuracy: 0.9585\n",
      "Epoch 46/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.1210 - sparse_categorical_accuracy: 0.9587\n",
      "Epoch 47/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.1191 - sparse_categorical_accuracy: 0.9603\n",
      "Epoch 48/50\n",
      "189/189 [==============================] - 61s 324ms/step - loss: 0.1173 - sparse_categorical_accuracy: 0.9615\n",
      "Epoch 49/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.1096 - sparse_categorical_accuracy: 0.9633\n",
      "Epoch 50/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 0.1052 - sparse_categorical_accuracy: 0.9655\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.9484 - sparse_categorical_accuracy: 0.7867\n",
      "Test accuracy: 78.67%\n"
     ]
    }
   ],
   "source": [
    "encoder = create_encoder()\n",
    "classifier = create_classifier(encoder)\n",
    "classifier.summary()\n",
    "\n",
    "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
    "\n",
    "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jcz47mPh99KK"
   },
   "source": [
    "## Experiment 2: Use supervised contrastive learning\n",
    "\n",
    "In this experiment, the model is trained in two phases. In the first phase,\n",
    "the encoder is pretrained to optimize the supervised contrastive loss,\n",
    "described in [Prannay Khosla et al.](https://arxiv.org/abs/2004.11362).\n",
    "\n",
    "In the second phase, the classifier is trained using the trained encoder with\n",
    "its weights freezed; only the weights of fully-connected layers with the\n",
    "softmax are optimized.\n",
    "\n",
    "### 1. Supervised contrastive learning loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xbgCr_ft99KK"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        # Compute logits\n",
    "        logits = tf.divide(\n",
    "            tf.matmul(\n",
    "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
    "\n",
    "\n",
    "def add_projection_head(encoder):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6is1Dsx99KK"
   },
   "source": [
    "### 2. Pretrain the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2Ac3V90t99KK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " cifar10-encoder (Functional  (None, 2048)             23564807  \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               262272    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,827,079\n",
      "Trainable params: 23,781,632\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "189/189 [==============================] - 63s 322ms/step - loss: 5.3210\n",
      "Epoch 2/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 5.0680\n",
      "Epoch 3/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 4.9143\n",
      "Epoch 4/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 4.7915\n",
      "Epoch 5/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 4.6760\n",
      "Epoch 6/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 4.5887\n",
      "Epoch 7/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 4.4993\n",
      "Epoch 8/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 4.4440\n",
      "Epoch 9/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 4.3747\n",
      "Epoch 10/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 4.3232\n",
      "Epoch 11/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 4.2631\n",
      "Epoch 12/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 4.2085\n",
      "Epoch 13/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 4.1606\n",
      "Epoch 14/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 4.1242\n",
      "Epoch 15/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 4.0810\n",
      "Epoch 16/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 4.0451\n",
      "Epoch 17/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 4.0115\n",
      "Epoch 18/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.9721\n",
      "Epoch 19/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.9399\n",
      "Epoch 20/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.9035\n",
      "Epoch 21/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.8855\n",
      "Epoch 22/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.8418\n",
      "Epoch 23/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.8281\n",
      "Epoch 24/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.7992\n",
      "Epoch 25/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.7748\n",
      "Epoch 26/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.7602\n",
      "Epoch 27/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.7371\n",
      "Epoch 28/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.7137\n",
      "Epoch 29/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.7016\n",
      "Epoch 30/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.6817\n",
      "Epoch 31/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.6546\n",
      "Epoch 32/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.6566\n",
      "Epoch 33/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.6321\n",
      "Epoch 34/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.6244\n",
      "Epoch 35/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.6089\n",
      "Epoch 36/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.5917\n",
      "Epoch 37/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.5847\n",
      "Epoch 38/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.5781\n",
      "Epoch 39/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.5664\n",
      "Epoch 40/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.5485\n",
      "Epoch 41/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.5571\n",
      "Epoch 42/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.5325\n",
      "Epoch 43/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.5267\n",
      "Epoch 44/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.5159\n",
      "Epoch 45/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.5185\n",
      "Epoch 46/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.5071\n",
      "Epoch 47/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.4988\n",
      "Epoch 48/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.4893\n",
      "Epoch 49/50\n",
      "189/189 [==============================] - 61s 322ms/step - loss: 3.4936\n",
      "Epoch 50/50\n",
      "189/189 [==============================] - 61s 323ms/step - loss: 3.4841\n"
     ]
    }
   ],
   "source": [
    "encoder = create_encoder()\n",
    "\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "encoder_with_projection_head.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=SupervisedContrastiveLoss(temperature),\n",
    ")\n",
    "\n",
    "encoder_with_projection_head.summary()\n",
    "\n",
    "history = encoder_with_projection_head.fit(\n",
    "    x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c64QSnOe99KK"
   },
   "source": [
    "### 3. Train the classifier with the frozen encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kL7koqC599KK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "189/189 [==============================] - 22s 108ms/step - loss: 0.1422 - sparse_categorical_accuracy: 0.9676\n",
      "Epoch 2/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0988 - sparse_categorical_accuracy: 0.9751\n",
      "Epoch 3/50\n",
      "189/189 [==============================] - 20s 106ms/step - loss: 0.1013 - sparse_categorical_accuracy: 0.9734\n",
      "Epoch 4/50\n",
      "189/189 [==============================] - 20s 106ms/step - loss: 0.0944 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 5/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0959 - sparse_categorical_accuracy: 0.9745\n",
      "Epoch 6/50\n",
      "189/189 [==============================] - 20s 106ms/step - loss: 0.0945 - sparse_categorical_accuracy: 0.9743\n",
      "Epoch 7/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0947 - sparse_categorical_accuracy: 0.9740\n",
      "Epoch 8/50\n",
      "189/189 [==============================] - 20s 106ms/step - loss: 0.0938 - sparse_categorical_accuracy: 0.9746\n",
      "Epoch 9/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0937 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 10/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0929 - sparse_categorical_accuracy: 0.9745\n",
      "Epoch 11/50\n",
      "189/189 [==============================] - 20s 106ms/step - loss: 0.0897 - sparse_categorical_accuracy: 0.9751\n",
      "Epoch 12/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0917 - sparse_categorical_accuracy: 0.9748\n",
      "Epoch 13/50\n",
      "189/189 [==============================] - 20s 106ms/step - loss: 0.0931 - sparse_categorical_accuracy: 0.9739\n",
      "Epoch 14/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0898 - sparse_categorical_accuracy: 0.9753\n",
      "Epoch 15/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0901 - sparse_categorical_accuracy: 0.9756\n",
      "Epoch 16/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0896 - sparse_categorical_accuracy: 0.9754\n",
      "Epoch 17/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0890 - sparse_categorical_accuracy: 0.9747\n",
      "Epoch 18/50\n",
      "189/189 [==============================] - 20s 108ms/step - loss: 0.0901 - sparse_categorical_accuracy: 0.9757\n",
      "Epoch 19/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0896 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 20/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0932 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 21/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0925 - sparse_categorical_accuracy: 0.9748\n",
      "Epoch 22/50\n",
      "189/189 [==============================] - 20s 108ms/step - loss: 0.0907 - sparse_categorical_accuracy: 0.9754\n",
      "Epoch 23/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0902 - sparse_categorical_accuracy: 0.9756\n",
      "Epoch 24/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0890 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 25/50\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 0.0863 - sparse_categorical_accuracy: 0.9747\n",
      "Epoch 26/50\n",
      "189/189 [==============================] - 20s 108ms/step - loss: 0.0879 - sparse_categorical_accuracy: 0.9761\n",
      "Epoch 27/50\n",
      "189/189 [==============================] - 20s 108ms/step - loss: 0.0892 - sparse_categorical_accuracy: 0.9755\n",
      "Epoch 28/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0901 - sparse_categorical_accuracy: 0.9758\n",
      "Epoch 29/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0932 - sparse_categorical_accuracy: 0.9741\n",
      "Epoch 30/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0901 - sparse_categorical_accuracy: 0.9756\n",
      "Epoch 31/50\n",
      "189/189 [==============================] - 20s 108ms/step - loss: 0.0878 - sparse_categorical_accuracy: 0.9755\n",
      "Epoch 32/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0891 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 33/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0871 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 34/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0931 - sparse_categorical_accuracy: 0.9737\n",
      "Epoch 35/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0902 - sparse_categorical_accuracy: 0.9748\n",
      "Epoch 36/50\n",
      "189/189 [==============================] - 20s 108ms/step - loss: 0.0886 - sparse_categorical_accuracy: 0.9760\n",
      "Epoch 37/50\n",
      "189/189 [==============================] - 20s 108ms/step - loss: 0.0921 - sparse_categorical_accuracy: 0.9742\n",
      "Epoch 38/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0939 - sparse_categorical_accuracy: 0.9737\n",
      "Epoch 39/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0903 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 40/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0865 - sparse_categorical_accuracy: 0.9757\n",
      "Epoch 41/50\n",
      "189/189 [==============================] - 20s 108ms/step - loss: 0.0921 - sparse_categorical_accuracy: 0.9746\n",
      "Epoch 42/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0912 - sparse_categorical_accuracy: 0.9754\n",
      "Epoch 43/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0931 - sparse_categorical_accuracy: 0.9746\n",
      "Epoch 44/50\n",
      "189/189 [==============================] - 21s 110ms/step - loss: 0.0902 - sparse_categorical_accuracy: 0.9743\n",
      "Epoch 45/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0874 - sparse_categorical_accuracy: 0.9756\n",
      "Epoch 46/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0906 - sparse_categorical_accuracy: 0.9757\n",
      "Epoch 47/50\n",
      "189/189 [==============================] - 20s 108ms/step - loss: 0.0902 - sparse_categorical_accuracy: 0.9747\n",
      "Epoch 48/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0883 - sparse_categorical_accuracy: 0.9747\n",
      "Epoch 49/50\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 0.0887 - sparse_categorical_accuracy: 0.9752\n",
      "Epoch 50/50\n",
      "189/189 [==============================] - 20s 108ms/step - loss: 0.0885 - sparse_categorical_accuracy: 0.9755\n",
      "313/313 [==============================] - 8s 23ms/step - loss: 1.0334 - sparse_categorical_accuracy: 0.7940\n",
      "Test accuracy: 79.4%\n"
     ]
    }
   ],
   "source": [
    "classifier = create_classifier(encoder, trainable=False)\n",
    "\n",
    "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
    "\n",
    "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gup-BLsm99KK"
   },
   "source": [
    "We get to an improved test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWVlLQqh99KK"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "As shown in the experiments, using the supervised contrastive learning technique\n",
    "outperformed the conventional technique in terms of the test accuracy. Note that\n",
    "the same training budget (i.e., number of epochs) was given to each technique.\n",
    "Supervised contrastive learning pays off when the encoder involves a complex\n",
    "architecture, like ResNet, and multi-class problems with many labels.\n",
    "In addition, large batch sizes and multi-layer projection heads\n",
    "improve its effectiveness. See the [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n",
    "paper for more details.\n",
    "\n",
    "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/supervised-contrastive-learning-cifar10)\n",
    "and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/supervised-contrastive-learning)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "supervised-contrastive-learning",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
